{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class work 1: Introduction to training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that we've covered some data science basics, let's get a very baseline neural network training pipeline going. \n",
    "\n",
    "**Important:** Make sure this notebook is pointing toward your custom `conda` kernel (top right drop-down menu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_0'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">0. Installing pytorch</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl (888.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.0/888.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.23.0-cp310-cp310-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 KB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.7.1\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.13.1.3\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.8.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.8.93\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in /home/amand/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.3.83\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.9.90\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.8.90\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.3.90\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.4.0\n",
      "  Downloading triton-3.4.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.8.93\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.10.2.21\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.8.93\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /home/amand/.local/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Collecting nvidia-nccl-cu12==2.27.3\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.4.0->torch) (59.6.0)\n",
      "Requirement already satisfied: numpy in /home/amand/.local/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/amand/.local/lib/python3.10/site-packages (from torchvision) (9.5.0)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "Successfully installed filelock-3.19.1 fsspec-2025.9.0 mpmath-1.3.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.8.0 torchvision-0.23.0 triton-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3929, 0.9546, 0.9154],\n",
      "        [0.5970, 0.6793, 0.8816],\n",
      "        [0.2915, 0.6950, 0.5424],\n",
      "        [0.0468, 0.5761, 0.3842],\n",
      "        [0.2035, 0.4470, 0.2181]])\n"
     ]
    }
   ],
   "source": [
    "### test that torch will work \n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a full cell of imports. What's a good resolution we can use for plotting? Refer to the previous in-class notebook if you can't remember, and fill it in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### here's the full cell of imports \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### set plot resolution\n",
    "%config InlineBackend.figure_format = ??? # <<<<<<<<<<<< INSERT CODE HERE\n",
    "\n",
    "### set default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc('font', size=medium_size)          # default text sizes\n",
    "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n",
    "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n",
    "plt.rc('legend', fontsize=medium_size)    # legend\n",
    "plt.rc('axes', titlesize=large_size)      # axes title\n",
    "plt.rc('axes', labelsize=large_size)      # x and y labels\n",
    "plt.rc('figure', titlesize=large_size)    # figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_1'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">1. Generating a dataset</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll generate one of the simplest toy datasets there is: two Gaussian distributions. Let's say that they describe our signal and our background events, but that there are only 2 parameters we can measure ($\\theta_1$ and $\\theta_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Two 2D Gaussian distributions with the same number of events\n",
    "n_events = 2000\n",
    "\n",
    "### Background is sampled from N(mu0, Sigma0)\n",
    "mu0 = np.array([-1.0, -1.0])\n",
    "cov0 = np.array([[1.0, 0.2],\n",
    "                 [0.2, 1.2]])\n",
    "x0 = np.random.multivariate_normal(mu0, cov0, size=n_events)\n",
    "y0 = np.zeros((n_events, 1), dtype=np.float32)\n",
    "\n",
    "### Signal is sampled from N(mu1, Sigma1)\n",
    "mu1 = np.array([1.0, 1.0])\n",
    "cov1 = np.array([[1.3, -0.3],\n",
    "                 [-0.3, 0.8]])\n",
    "x1 = np.random.multivariate_normal(mu1, cov1, size=n_events)\n",
    "y1 = np.ones((n_events, 1), dtype=np.float32)\n",
    "\n",
    "X = np.vstack([x0, x1]).astype(np.float32)\n",
    "y = np.vstack([y0, y1]).astype(np.float32)\n",
    "\n",
    "### Put these together and shuffle\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx] # inputs\n",
    "y = y[idx] # labels\n",
    "\n",
    "print(\"Data shapes:\", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it into a Pandas dataframe for legibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.hstack([X,y]), columns = [\"theta_1\", \"theta_2\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(df[df.label != True].theta_1, df[df.label != True].theta_2, s=6, color=\"grey\", alpha=0.5, label=\"Background\")\n",
    "plt.scatter(df[df.label == True].theta_1, df[df.label == True].theta_2, s=6, color=\"crimson\", alpha=0.5, label=\"Signal\")\n",
    "plt.legend()\n",
    "plt.title(\"Visualizing our datasets\")\n",
    "plt.xlabel(r\"$\\theta_1$\"); plt.ylabel(r\"$\\theta_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_2'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">2. Defining a neural network model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=???, hidden_dim=32): # <<<< FILL IN INPUT DIMENSION \n",
    "        super().__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # output is 1-dimensional for binary classification\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.MLP(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_3'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">3. Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key steps include splitting into train/validation/test sets, standardizing data, and building dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"label\"])  # only look at the features, not the labels\n",
    "y = df[\"label\"]                 # labels\n",
    "\n",
    "### Take the first 70% for training\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(\n",
    "    X, y, test_size=0.3)\n",
    "\n",
    "### Then evenly split the remaining 30% into validation & test set \n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_valtest, y_valtest, test_size=0.5\n",
    ")\n",
    "\n",
    "print(f\"Train set has {len(X_train)} events.\")\n",
    "print(f\"Validation set has {len(X_val)} events.\")\n",
    "print(f\"Test set has {len(X_test)} events.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can standardize your data using a \"z-score\" manually by subtracting the mean and dividing by the standard deviation. \n",
    "\n",
    "Alternatively, you can use the StandardScaler plugin to do this automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "### IMPORTANT -- you only want to define your scaling (\"fit_transform\") based on your training dataset.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "### Now re-use the same transformation on validation & test sets (\"transform\")\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Train mean (before scaling):\", np.array(X_train.mean(axis=0)))\n",
    "print(\"Train mean (after scaling) -- each dimension should be close to 0:\", X_train_scaled.mean(axis=0))\n",
    "print(\"Train std (before scaling)\", np.array(X_train.std(axis=0)))\n",
    "print(\"Train std (after scaling) -- each dimension should be close to 1:\", X_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='problem_x'></a> \n",
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem: Standardizing with z-scores </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try implementing this standardization by hand, and check that your scaled distributions are close to the ones that you got using the StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_by_hand = ??? \n",
    "X_val_scaled_by_hand = ??? \n",
    "X_test_scaled_by_hand = ??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "bins = np.linspace(???, ??? , 40)\n",
    "### theta_1\n",
    "plt.hist(X_train_scaled[:,0], bins=bins, histtype=\"step\", linewidth=2, label=\"$\\theta_1$\");\n",
    "plt.hist(X_train_scaled_by_hand[:,0], bins=bins, histtype=\"step\", linewidth=2, label=\"$\\theta_1$ (by hand)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "bins = np.linspace(???, ??? , 40)\n",
    "### theta_2\n",
    "plt.hist(X_train_scaled[:,1], bins=bins, histtype=\"step\", linewidth=2, label=\"$\\theta_2$\");\n",
    "plt.hist(X_train_scaled_by_hand[:,1], bins=bins, histtype=\"step\", linewidth=2, label=\"$\\theta_2$ (by hand)\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we'll construct data loaders. we'll use 32 as our default batch size for now. \n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_scaled), torch.from_numpy(y_train.values.astype(\"float32\")).view(-1, 1))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val_scaled),   torch.from_numpy(y_val.values.astype(\"float32\")).view(-1, 1))\n",
    "test_ds  = TensorDataset(torch.from_numpy(X_test_scaled),  torch.from_numpy(y_test.values.astype(\"float32\")).view(-1, 1))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test one of these loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_batch, y_batch in train_loader:\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_4'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">4. Training</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with tracking our model training using `livelossplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for available GPUs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "### Initialize the model, optimizer, and loss function -- what should the input dimensionality be?\n",
    "model = MLP(input_dim=???, hidden_dim=32).to(device) #<<<<<<<< FILL IN HERE\n",
    "model = model.to(device) # move onto the GPU, if present\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=??? ) #<<<<<<<< FILL IN HERE -- what's the default learning rate for Adam?\n",
    "\n",
    "liveloss = PlotLosses(figsize=(9, 4)) \n",
    "logs = {}\n",
    "\n",
    "### Training loop\n",
    "for epoch in range(10):\n",
    "\n",
    "    ### train\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Move the batch onto the GPU, if present\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        out = model(x_batch)\n",
    "        loss = loss_fn(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    train_loss_per_batch = total_train_loss / len(train_loader)\n",
    "    logs['loss'] = train_loss_per_batch\n",
    "\n",
    "    ### validate\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        \n",
    "        ### Move the batch onto the GPU, if present\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        out = model(x_batch)\n",
    "        loss = loss_fn(out, y_batch)\n",
    "        total_val_loss += loss.item()\n",
    "    \n",
    "    val_loss_per_batch = total_val_loss / len(val_loader)\n",
    "    logs['val_loss'] = val_loss_per_batch\n",
    "    \n",
    "    liveloss.update(logs)\n",
    "    \n",
    "    liveloss.send()\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip this for now, but if you have time once you finish this notebook, try connecting the notebook to Wandb to have the option of tracking your runs: \n",
    "- https://docs.wandb.ai/guides/track/jupyter/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "# ??? <<<< FILL IN HERE\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_5'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">5. Evaluating model performance</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss alone might not be a very intuitive metric for understanding our model performance. Let's evaluate the model on our holdout test set and get some new metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set the threshold for your NN score (what value makes sense here? can you plot values of your NN scores to understand their distribution?) \n",
    "threshold = ??? # <<<< FILL IN HERE\n",
    "\n",
    "### test set \n",
    "model.eval()\n",
    "test_loss, test_acc, count = 0.0, 0.0, 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        \n",
    "        ### Move the batch onto the GPU, if present\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "    \n",
    "        out = model(x_batch)\n",
    "        loss = loss_fn(out, y_batch)\n",
    "        test_loss += loss.item() * x_batch.size(0)\n",
    "        test_acc  += (torch.sigmoid(out) > threshold).float().eq(y_batch).float().mean().item() * x_batch.size(0)\n",
    "        count += x_batch.size(0)\n",
    "        \n",
    "print(f\"Average test loss: {test_loss/count:.4f} | Test accuracy: {test_acc/count:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further exercises:\n",
    "- Try modifying the hidden size and number of MLP layers, to see the effect on performance\n",
    "- Try increasing the dataset size\n",
    "- Try different learning rates\n",
    "- What's the effect of changing your batch size?\n",
    "- Try adding dropout\n",
    "- Try making the task harder by making the Gaussians overlap and/or shifting their positions. Where does your model start to break down?\n",
    "- Can you track an \"accuracy\" metric alongside the loss in your live tracking plots?\n",
    "- Can you track your runs on Wandb? \n",
    "- What is a simple baseline you could define here to get context for how well your model is doing? Implement this and use it as a comparison.\n",
    "- Can you visualize the decision boundary in parameter space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNjh80VUTQo5GkMNUc0DRQc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
